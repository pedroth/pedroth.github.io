<html>
<head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NQKJHP3');</script>
<!-- End Google Tag Manager -->
    <style>
        #experimentsTab {
        display: none
        }

        #projectTitle {
        color : rgb(255,255,255)
        }

        #header{
        background: #212121;
        background: -moz-linear-gradient(top, #373737, #212121);
        background: -webkit-linear-gradient(top, #373737, #212121);
        background: -ms-linear-gradient(top, #373737, #212121);
        background: -o-linear-gradient(top, #373737, #212121);
        background: linear-gradient(top, #373737, #212121);
        }

        #footer{
        background: #212121;
        background: -moz-linear-gradient(top, #373737, #212121);
        background: -webkit-linear-gradient(top, #373737, #212121);
        background: -ms-linear-gradient(top, #373737, #212121);
        background: -o-linear-gradient(top, #373737, #212121);
        background: linear-gradient(top, #373737, #212121);
        color: #fff;
        padding: 50px 10px 5px 10px;
        }

        #main_content{
            padding-right: 1px;
            padding-left: 1px;
        }

        .tr-caption-container {
            margin-top: 10px;
            margin-bottom: 10px;
        }
    </style>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="VisualExperiments : ">
    <script type="text/x-mathjax-config">
	  	MathJax.Hub.Config({
	    extensions: ["tex2jax.js"],
	    jax: ["input/TeX", "output/HTML-CSS"],
	    tex2jax: {
	      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
	      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
	      processEscapes: true
	    },
	    "HTML-CSS": { availableFonts: ["TeX"] }
	  });
    </script>

    <script defer src="https://use.fontawesome.com/releases/v5.0.13/js/solid.js" integrity="sha384-tzzSw1/Vo+0N5UhStP3bvwWPq+uvzCMfrN1fEFe+xBmv1C/AtVX5K0uZtmcHitFZ" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.0.13/js/fontawesome.js" integrity="sha384-6OIrr52G08NpOFSZdxxz1xdNSndlD4vdcf/q2myIUVO0VsqaGHJsB0RaBE01VTOY" crossorigin="anonymous"></script>
    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
            type='text/javascript'></script>
    <!-- Jquery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.11.2/css/bootstrap-select.min.css">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-select/1.11.2/js/bootstrap-select.min.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NQKJHP3"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div id="header" class="jumbotron text-center">
    <a href="https://pedroth.github.io/visualExperiments/"><h1 id="projectTitle">VisualExperiments</h1></a>
    <ul class="list-inline">
        <li>
            <h3><a href="javascript:void(0)" id="experimentsName">Experiments</a></h3>
        </li>
        <li><h3><a href="https://pedroth.github.io/visualExperiments/Blog/Blog.html">Blog</a></h3></li>
        <li><h3><a href="https://www.youtube.com/user/peterluigi/">Youtube</a></h3></li>
        <li><h3><a href="https://www.khanacademy.org/profile/pedroth/programs">Khan Academy</a></h3></li>
    </ul>
    <div>
        <ul class="list-inline" id="experimentsTab">
            <li><h3><a href="https://pedroth.github.io/visualExperiments/JsExperiments/JsExperiments.html">
                JsExperiments </a></h3></li>
            <li><h3><a href="https://pedroth.github.io/visualExperiments/JavaExperiments/JavaExperiments.html">
                JavaExperiments </a></h3></li>
        </ul>
    </div>
    <script> $("#experimentsName").click( function() { $("#experimentsTab").slideToggle();})</script>
</div>

<div class="container">


<h1>GradientJacobianHessian</h1>
<header>
    <div class="container">
        <div class="row">
            <div class="col-lg-auto col-md-auto mx-auto">
                <div class="post-heading">
                    <h1>Gradient, Jacobian and Hessian</h1>
                </div>
            </div>
        </div>
    </div>
</header>
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-auto col-md-auto mx-auto">
                <h2> Intro </h2>

                One variable differential calculus is a essential tool to study non-linear functions. Why ? Because the differential framework provides a local linear approximation to the function (tangent line to the function graph), these approximations are important since linear functions are easy to understand.
                Thus this framework enables us to study non-linear functions by studying local linear approximations. These linear approximations determines properties of the function locally, such as rates of growth of a function. Also because we can find closed form expressions of the first order derivatives, we can study non-linear function in the entire domain.
                We can also apply differential calculus to the derivative function, giving us better local approximations of the functions in terms of easy to understand functions like polynomials (check <a href="http://pedroth.github.io/visualExperiments/Blog/TaylorPolynomial/TaylorPolynomial.html" target="_blank">Taylor series</a>).
                One variable differential calculus is essential to many areas such as differential equations and optimization theory (and these are super useful in science and engineering). In this post we will study how to extend the differential calculus with more than one derivative, using just one-dimensional calculus.<br><br>


                <h2> Multi-variable functions </h2>

                A general multi-variable function is just a map $\mathbf{f}: \mathbb{R}^n\rightarrow \mathbb{R}^m$.
                Functions like function $\mathbf{f}$ can be seen as a vector where each coordinate is just a function $f_i:\mathbb{R}^n\rightarrow\mathbb{R}$.

                $$\mathbf{f}(\mathbf{x}) = \begin{bmatrix}f_1(\mathbf{x})\\\dots\\f_m(\mathbf{x})\end{bmatrix}$$

                There are multiple ways of visualizing such functions, but most of those require higher dimensional eyes, something that we don't have.
                Still we can built an intuition by looking for lower values of $n$ and $m$. If $n=2=m$ then we can visualize this function as a vector field, where for each $\mathbf{x}$ we stick a arrow $\mathbf{f(x)}$. We can also see this functions as a deformations of an original space of inputs to the space of outputs (imagine a grid being deformed).
                You can try to generalize these two visualizations for different values of $n$ and $m$. There is still the more classic graph of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ where we add one more perpendicular axis to the original n-dimensional space and then deform the space through this new dimension according to $f$ (using the deformation given by the pair $(\mathbf{x}, f(\mathbf{x}))$.
                Below you can visualize most of these visualizations of functions for low dimensions:

                <br><br>
                <button class="btn btn-primary" onclick="app.run(0)"><h4> $f:\mathbf{R^2} \rightarrow  \mathbb{R}^3$ </h4></button>
                <div id="sim1" style="display:none; text-align: center">
                    <canvas id="grid" width="500" height="500"></canvas>
                    <canvas id="surface" width="500" height="500"></canvas>
                    <br><br>
                    <div align="center" class="container">
                        <button class="btn btn-primary" onclick="app.apply(0, x => {})">
                            generate function
                        </button>
                    </div>
                    <div style="text-align: center;">
                        <p>lero lero</p>
                    </div>
                </div>

                <br><br>
                <button class="btn btn-primary" onclick="app.run(1)"><h4> Vector field $f:\mathbf{R^2} \rightarrow  \mathbb{R}^2$ </h4></button>
                <div id="sim2" style="display:none; text-align: center">
                    <canvas id="vectorField" width="650" height="500"></canvas>
                    <div align="center" class="container">
                        <button class="btn btn-primary" onclick="app.apply(1, x => {})">
                            generate function
                        </button>
                    </div>
                    <div style="text-align: center;">
                        <p>Lero Lero.</p>
                    </div>
                </div>

                <br><br>
                <button class="btn btn-primary" onclick="app.run(1)"><h4> Graph $f:\mathbf{R^2} \rightarrow  \mathbb{R}$ </h4></button>
                <div id="sim3" style="display:none; text-align: center">
                    <canvas id="grid2" width="500" height="500"></canvas>
                    <canvas id="graph" width="500" height="500"></canvas>
                    <br><br>
                    <div align="center" class="container">
                        <button class="btn btn-primary" onclick="app.apply(2, x => {})">
                            generate function
                        </button>
                    </div>
                    <div style="text-align: center;">
                        <p>lero lero</p>
                    </div>
                </div>

                <br><br>
                <button class="btn btn-primary" onclick="app.run(1)"><h4> Graph $f:\mathbf{R^2} \rightarrow  \mathbb{R}$ </h4></button>
                <div id="sim4" style="display:none; text-align: center">
                    <canvas id="grid3" width="500" height="500"></canvas>
                    <canvas id="line" width="500" height="500"></canvas>
                    <br><br>
                    <div align="center" class="container">
                        <button class="btn btn-primary" onclick="app.apply(3, x => {})">
                            generate function
                        </button>
                    </div>
                    <div style="text-align: center;">
                        <p>lero lero</p>
                    </div>
                </div>

            </div>
        </div>
    </div>
</article>
<script src="GradientJacobianHessian.js"></script>






<h2>
<span style="font-size: small;">&nbsp;</span><br />
<span style="font-size: small;"> </span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Let $f(x,y):\mathbb{R}^2\rightarrow\mathbb{R}$ be a multivariate function and $\mathbf{g}(t):\mathbb{R}\rightarrow\mathbb{R}^2$&nbsp; a curve in $\mathbb{R}^2$, then we can study $f$ using one variable calculus by constraining the domain of $f$ to $\mathbf{g}$ curve, using function composition $f\left ( g(t) \right )$. For simplification, let $\mathbf{g}(t) = \mathbf{a} + \mathbf{v}t$ be a line starting at $\mathbf{a}$ with $\mathbf{v}$ direction ($\mathbf{a,v}\in \mathbb{R}^2$). With $f(\mathbf{g}(t))$ we can take the usual one variable derivative.</span><br />
<span style="font-size: small;">$$\frac{d}{dt}f(\mathbf{g}(0)) = \lim_{t\rightarrow0}\frac{f(\mathbf{g}(\mathbf{a+v}t))-f(\mathbf{g}(\mathbf{a}))}{t}\;\;\;\;(1)$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">The left hand side of (1) tells us that we can use all machinery of one dimensional calculus, while the right site tell us a much more numerical approach to compute the derivative, this provide us the visualization to pursue this subject further.(check fig 1)</span><br />
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-B3FxyI9q9Ho/VZUvI8CmDVI/AAAAAAAAAU8/bcIVRWLUBok/s1600/Numerozinhos_6.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="400" src="https://3.bp.blogspot.com/-B3FxyI9q9Ho/VZUvI8CmDVI/AAAAAAAAAU8/bcIVRWLUBok/s640/Numerozinhos_6.jpg" width="640" /></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;"><span style="font-size: small;">Fig 1</span></td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td class="tr-caption" style="text-align: center;"></td></tr>
</tbody></table>
<span style="font-size: small;">Let us define $$\frac{\partial f}{\partial \mathbf{v}} = \lim_{t\rightarrow 0}
\frac{f(\mathbf{a+v}t)-f(\mathbf{a})}{t}\;\;, \forall
\,\mathbf{v}\in\mathbb{R}^2$$.</span><br />
<span style="font-size: small;"><br /></span>
<br />
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Example : let $f(x,y) = \sin(x+y)$ and
$\mathbf{g}(t) = [1+t, 1+t]^T$ then $$\frac{d(f\circ g)(t)}{dt} =
\frac{d(\sin(1+t+1+t))}{dt} = 2\cos(2+2t)$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Now we can study a multivariate function
by watching its derivative at each direction. We want to find expression for
$\frac{\partial f}{\partial \mathbf{v}}$ in terms of $x$ and $y$, for that we
will use the same approach as one-dimensional calculus. We will find a linear
function that best approximates function $f$ at $\mathbf{a}=[a,b]^T$. Let
$p(x,y) = p_0 + m_x (x-a) + m_y(y-b)$ be &nbsp;a such function. We need to find
a $p_0, m_x$ and $m_y$ such that $$p(x,y) + r(x-a,y-b) = f(x,y)\;\;\;(2)$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Where $\lim_{\mathbf{x}\rightarrow
\mathbf{a}}r(\mathbf{x-a}) = 0$. We get $p_0$ by substituting $\mathbf{x} =
\mathbf{a}$ on (2).<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">$$f(\mathbf{a}) = p_0 + m_x(a-a) +
m_y(b-b) + r(0,0) = p_0$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Doing some simple algebra we get $m_x$ and
$m_y$ in terms of $f$.<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span lang="PT" style="font-size: small;">$$f(a+h,b) - f(a,b) =
m_x h + r(h,0)$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span lang="PT" style="font-size: small;">$$\frac{f(a+h,b)-f(a,b)}{h}
= m_x +&nbsp;\frac{r(h,0)}{h}$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span lang="PT" style="font-size: small;">$$\lim_{h\rightarrow
0}\frac{f(a+h,b)-f(a,b)}{h} = m_x + \lim_{h\rightarrow0}\frac{r(h,0)}{h}$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">$$\frac{\partial f}{\partial x}(a,b) =
m_x$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Similarly $m_y =&nbsp;\frac{\partial
f}{\partial y}(a,b)&nbsp;$. Note this is not a complete proof, here we just
want to get an intuition. Now we can compute $\frac{d (f \circ
\mathbf{g})}{dt}$ in terms of $\mathbf{x}$ as follow:<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">$$\frac{d (f \circ \mathbf{g})}{dt}(t) =
\lim_{h \rightarrow 0}\frac{f(\mathbf{g}(t+h))-f(\mathbf{g}(t))}{h}&nbsp;$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Using linear approximation, first in $t$ then in $(x,y)$</span><br />
<br /></div>
<span style="font-size: small;">$$\Leftrightarrow \lim_{h \rightarrow 0}\frac{f\left ( \mathbf{g}(t) + \frac{d\mathbf{g}}{dt}(t) h \right)-f(\mathbf{g}(t))}{h} = \lim_{h\rightarrow 0}\frac{f(\mathbf{g}(t)) + f_x(\mathbf{g}(t)) \times (a + v_1h - a) + f_y(\mathbf{g}(t)) \times (b + v_2h -b) - f(\mathbf{g}(t))}{h}$$</span><br />
<span style="font-size: small;"><br /></span>$$\Leftrightarrow \lim_{h \rightarrow 0}\frac{f\left ( \mathbf{g}(t) + \frac{d\mathbf{g}}{dt}(t) h \right)-f(\mathbf{g}(t))}{h} = \lim_{h\rightarrow 0}\frac{f_x(\mathbf{g}(t)) \times v_1h + f_y(\mathbf{g}(t)) \times v_2h}{h}$$<br />
<br />
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Note that $\mathbf{g}(t) = \mathbf{a} +
\mathbf{v}t$, where $\mathbf{v} = [v_1, v_2]^T$. Also $f_x = \frac{\partial
f}{\partial x} = \frac{d f}{dx}$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Hence, $$\frac{d (f \circ \mathbf{g})}{dt}(t_0) = f_x(\mathbf{g}(t_0)) \times v_1 + f_y(\mathbf{g}(t_0)) \times v_2 = \nabla \mathbf{f}^T \left ( \mathbf{g}(t_0) \right )\mathbf{v}$$</span></div>
<div style="margin: 0in 0in 0.0001pt;">
<br />
<span style="font-size: small;">Where $\nabla \mathbf{f}\left( \mathbf{g}(t_0)\right) = [f_x\left( \mathbf{g}(t_0)\right), f_y\left( \mathbf{g}(t_0)\right)]^T$ , is the gradient vector of $f$ at $\mathbf{g}(t_0)$.</span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Remark: In this context $\mathbf{u}^T
\mathbf{v} = &lt;\mathbf{u} ,\mathbf{v}&gt; =\mathbf{u} \cdot \mathbf{v}$
&nbsp;&nbsp;<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<br />
<div style="margin: 0in 0in 0.0001pt;">
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-ar38JL0uLuE/Vaqeeej6pQI/AAAAAAAAAWs/evVAz5VKUpw/s1600/Numerozinhos_08.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="400" src="https://4.bp.blogspot.com/-ar38JL0uLuE/Vaqeeej6pQI/AAAAAAAAAWs/evVAz5VKUpw/s640/Numerozinhos_08.jpg" width="640" /></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Fig 2</td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody></tbody></table>
</div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Now with this tool we can study nonlinear multivariable functions. For instance, let f $\mathbb{R}^n \rightarrow
\mathbb{R}$ ,we wish to study in which directions of the domain we should go in
order to increase/decrease $f$ or to stay at a constant value. To find such
answers we just need to study the derivative of $f$ for various directions
$\mathbf{v} \in \mathbb{R}^n \;\; , |\mathbf{v}| = 1&nbsp;$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">$$\frac{\partial f}{\partial
\mathbf{v}}(\mathbf{a}) = \left &lt; \nabla \mathbf{f}(\mathbf{a}), \mathbf{v}
\right&gt;\;\;\;(3)$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">We know from&nbsp;the<span class="apple-converted-space">&nbsp;</span><a href="http://pedroth.github.io/visualExperiments/Blog/ProjectionDotProduct/ProjectionDotProduct.html" target="_blank">dot product</a>&nbsp;that $\left |\mathbf{proj_v\nabla
f}\right| = \left &lt;\mathbf{\nabla f}, \mathbf{v} \right&gt;$, then we know
that &nbsp;when $\mathbf{v} $ is parallel &nbsp;to $\mathbf{\nabla f}$ with the
same direction (3) is maximum, parallel with opposite direction (3) is minimum. When $\mathbf{v}$ is prependicular to $\mathbf{\nabla f}$ then (3)
is equal to zero. These properties are easy to check with the next
formula:&nbsp;<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">$$\left &lt;\mathbf{\nabla f}, \mathbf{v}
\right&gt; = \left | &nbsp;\mathbf{\nabla f}\right | \left | \mathbf{v} \right
| \cos \theta$$<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Where $\theta$ is the angle between
$\mathbf{\nabla f}$ and $\mathbf{v}$.<o:p></o:p></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Since the gradient its defined everywhere(as long as the function is <a href="https://en.wikipedia.org/wiki/Differentiable_function" target="_blank">differentiable</a>), the gradient can be seen as a vector field on the domain of the function(check fig 3). This gradient vector field points to local maxima of a function (this is easy to check from the argument above). While a orthogonal vector field to the gradient, are tangent vectors to the function level sets(points in the function domain with the same value).</span></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;"><br /></span></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><span style="font-size: small;"><a href="http://4.bp.blogspot.com/-Bp1kc0BbREg/VZ5KLUF22RI/AAAAAAAAAV4/f7wgJ8Dr4i0/s1600/Gradient_Visual.svg.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="480" src="https://4.bp.blogspot.com/-Bp1kc0BbREg/VZ5KLUF22RI/AAAAAAAAAV4/f7wgJ8Dr4i0/s640/Gradient_Visual.svg.png" width="640" /></a></span></td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td class="tr-caption" style="text-align: center;"><span style="font-size: small;">Fig 3 - Gradient vector field (from <a href="https://en.wikipedia.org/wiki/Gradient" target="_blank">wikipedia</a>)</span></td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody></tbody></table>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Other advantage of the gradient is that
provide a local linear approximation of $f$ (we know that because the gradient
was built for this purpose). Graphically this linear approximation is the
tangent hyper-plane to the graph of the function(check fig 2).<o:p></o:p></span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Linear approximation of a function at $\mathbf{x}_0$</span><br />
<span style="font-size: small;"><br /></span>
$$f(\mathbf{x}) = f(\mathbf{x_0} + (\mathbf{x}-\mathbf{x}_0) ) = f(\mathbf{x_0}) + \nabla \mathbf{f}(\mathbf{x_0}) ^T (\mathbf{x} - \mathbf{x_0})$$<br />
<br /></div>
<div style="margin: 0in 0in 0.0001pt;">
<span style="font-size: small;">Jacobian is easy to define with the
gradient.<o:p></o:p></span></div>
<span style="font-size: small;">&nbsp; Let $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$. $$\mathbf{f(x)} = \begin{bmatrix}<br /><br />f_1(\mathbf{x})\\<br /><br />\dots\\<br /><br />f_m(\mathbf{x})<br /><br />\end{bmatrix}$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Then the jacobian is the derivative of this function, this derivative becomes clear with the linear approximation of $\mathbf{f}$ around a point.</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Linear approximation of $\mathbf{f}$ around point $\mathbf{a}$ is just the linear approximation of each coordinate function $f_i\;\;i\in\{1,\dots,m\}$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$\mathbf{f(x)} \simeq&nbsp; \mathbf{f(a)} + \begin{bmatrix}<br /><br />\nabla \mathbf{f}_1(\mathbf{a})^T\\<br /><br />\dots<br /><br />\\<br /><br />\nabla \mathbf{f}_m(\mathbf{a})^T<br /><br />\end{bmatrix}(\mathbf{x-a}) \;\;\;(4)$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">The jacobian is the matrix at (4) with gradient vectors in its rows. With this definition, it is hard to get a good intuition of what is the jacobian. To provide such intuition I will give an example. Let $\mathbf{g}(u,v):\mathbb{R}^2\rightarrow \mathbb{R}^3$. $\mathbf{g}$ &nbsp;defines a surface in $\mathbb{R}^3$. Imagine that we want to find a tangent plane at $\mathbf{g}(u_0,v_0)$, to find this plane we need two non parallel tangent vector to the surface.Using two non intersecting curves on the surface, we can calculate a tagent vector to each curve, hence finding tangent vectors to the surface. We obtain these tangent vector as follows:</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$\lim_{h\rightarrow0} \frac{1}{h} \left[ \mathbf{g}(u_0+h,v_0)-\mathbf{g}(u_0,v_0)\right ] = \frac{\partial \mathbf{g}}{\partial u}(u_0,v_0)\\$$</span>
<span style="font-size: small;">and</span>
<span style="font-size: small;">$$ \lim_{h\rightarrow0} \frac{1}{h} \left[ \mathbf{g}(u_0,v_0 + h)-\mathbf{g}(u_0,v_0)\right ] = \frac{\partial \mathbf{g}}{\partial v}(u_0,v_0) $$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Now a tangent plane on $\mathbf{g}(u_0,v_0)$ is just a linear combination of the tangent vectors(check fig 4)&nbsp;</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$\mathbf{p}(u,v) = \mathbf{g}(u_0,v_0) + \frac{\partial \mathbf{g}}{\partial u}(u_0,v_0) (u-u_0) + \frac{\partial \mathbf{g}}{\partial v}(u_0,v_0) (v-v_0)$$</span>
<span style="font-size: small;">On matrix form</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;"><br /></span>
$$\mathbf{p}(u,v) = \mathbf{g}(u_0,v_0) + \begin{bmatrix}\frac{\partial \mathbf{g}(u_0,v_0)}{\partial u} &amp; \frac{\partial \mathbf{g}(u_0,v_0)}{\partial v}<br />
\end{bmatrix}\begin{bmatrix}<br />
u-u_0\\<br />
v-v_0<br />
\end{bmatrix}\;\;\; (5)$$<br />
<br />
$$\mathbf{p}(u,v) = \mathbf{g}(u_0,v_0) +<br />
\begin{bmatrix}<br />
\nabla\mathbf{g_1}(u_0,v_0)^T \\ \nabla\mathbf{g_2}(u_0,v_0)^T<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
u-u_0\\<br />
v-v_0<br />
\end{bmatrix}\;\;\;(5.5)$$<br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">The &nbsp;matrix on (5) is the jacobian of &nbsp;$\mathbf{g}$. It is the same matrix as in (4), but in (5) we see the columns instead of the rows. In (5.5) we see the rows of the matrix. The jacobian of $\mathbf{g}$ may be written as $\mathbf{J_g}(u,v) = D\mathbf{g}(u,v)$.</span><br />
<span style="font-size: small;"><br /></span>
<br />
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><span style="font-size: small;"><a href="http://4.bp.blogspot.com/-dCHTIxv4zes/VaqeercFaCI/AAAAAAAAAWw/xRg3NkgEYto/s1600/Numerozinhos_09.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="400" src="https://4.bp.blogspot.com/-dCHTIxv4zes/VaqeercFaCI/AAAAAAAAAWw/xRg3NkgEYto/s640/Numerozinhos_09.jpg" width="640" /></a></span></td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td class="tr-caption" style="text-align: center;"><span style="font-size: small;">Fig 4</span></td></tr>
</tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"></table>
<span style="font-size: small;">The Hessian is just the jacobian of the gradient function, e.g. $\mathbf{p(x)} = \mathbf{\nabla f(x)}$, then the Hessian is $\mathbf{J_p(x)}$. Here I donÂ´t want to describe the Hessian, I want to give the intuition of the hessian and show why it is interesting. The hessian is interesting because it provides a second order approximation to functions (towards a Taylor polynomial in the multivariable case), and I will show how to do it.</span><span style="font-size: small;"><br /></span>
<span style="font-size: small;">Let $f(\mathbf{x}):\mathbb{R}^n \rightarrow \mathbb{R}$, then we know form one dimensional calculus that:</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$f(\mathbf{a + v}t) = f(\mathbf{a}) + \int_0^t \frac{df}{dt}(\tau) \;d\tau$$</span><br />
<span style="font-size: small;">$$\Leftrightarrow f(\mathbf{a + v}t) = f(\mathbf{a}) +\int_0^t \left&lt;\nabla \mathbf{f(a+v}\tau),\mathbf{v}\right&gt; d\tau\;\;\;(6)$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$\left&lt;\nabla \mathbf{f(a+v}t),\mathbf{v}\right&gt; = \left&lt;\nabla \mathbf{f(a)},\mathbf{v}\right&gt; + \int_0^t \frac{d(\left&lt;\nabla \mathbf{f(a+v}\tau),\mathbf{v}\right&gt;)}{dt}\,d\tau$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$\Leftrightarrow \left&lt;\nabla \mathbf{f(a+v}t),\mathbf{v}\right&gt; = \left&lt;\nabla \mathbf{f(a)},\mathbf{v}\right&gt; &nbsp;+ \int_0^t \frac{d}{dt}\left( \frac{\partial f}{\partial x_1} v_1 + \dots + \frac{\partial f}{\partial x_n} v_n\right ) \;d\tau$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$ \Leftrightarrow \left&lt;\nabla \mathbf{f(a+v}t),\mathbf{v}\right&gt; = \left&lt;\nabla \mathbf{f(a)},\mathbf{v}\right&gt; &nbsp;+ \int_0^t \left &lt; \nabla\frac{\partial f}{\partial x_1},\mathbf{v}\right&gt; v_1 + \dots + \left &lt; \nabla\frac{\partial f}{\partial x_n},\mathbf{v} \right&gt; v_n \;d\tau $$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">With a little practice</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$ \Leftrightarrow \left&lt;\nabla \mathbf{f(a+v}t),\mathbf{v}\right&gt; = \left&lt;\nabla \mathbf{f(a)},\mathbf{v}\right&gt;&nbsp; + \int_0^t \mathbf{v}^T \begin{bmatrix}<br /><br />\nabla\frac{\partial f}{\partial x_1}^T \\<br /><br />\dots \\<br /><br />\nabla\frac{\partial f}{\partial x_n} ^T<br /><br />\end{bmatrix} \mathbf{v} \; d \tau \;\;\;(7) $$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">substituting (7) in (6) we get (note that this procedure is similar to the one we did on <a href="http://pedroth.github.io/visualExperiments/Blog/TaylorPolynomial/TaylorPolynomial.html" target="_blank">Taylor polynomial</a>)</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$f(\mathbf{a+v}t) = f(\mathbf{a}) + \int_0^t \left &lt; \nabla \mathbf{f}, \mathbf{v} \right &gt;\; d\tau + \int_0^t\int_0^\tau \left&lt; \mathbf{v}, \begin{bmatrix}<br /><br />\nabla \frac{\partial f}{\partial x_1}^T\\<br /><br />\dots\\<br /><br />\nabla \frac{\partial f}{\partial x_n}^T\\<br /><br />\end{bmatrix} \mathbf{v}\right&gt; \;d\tau_2\;d\tau$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">If we assume &nbsp;$\nabla \frac{\partial f(\mathbf{a + v}t)}{\partial x_i} = \nabla \frac{\partial f(\mathbf{a})} {\partial x_i} \;\; i \in \{1, \dots, n\}$ we get a local approximation of $f$.</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$f(\mathbf{a+v}t) \simeq f(\mathbf{a}) + \left &lt; \nabla \mathbf{f}, \mathbf{v} \right &gt;t + \left&lt; \mathbf{v}, \begin{bmatrix}<br /><br />\nabla \frac{\partial f}{\partial x_1}^T\\<br /><br />\dots\\<br /><br />\nabla \frac{\partial f}{\partial x_n}^T\\<br /><br />\end{bmatrix} \mathbf{v}\right&gt;\frac{t^2}{2} \;\;\; (8)$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">If $t = 1$ and $\mathbf{x = a + v}$ &nbsp;then (8) becomes</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">$$f(\mathbf{x}) \simeq f(\mathbf{a}) + \left &lt; \nabla \mathbf{f}, \mathbf{x-a} \right &gt; + \frac{1}{2}\left&lt; \mathbf{x-a}, \begin{bmatrix}<br /><br />\nabla \frac{\partial f}{\partial x_1}^T\\<br /><br />\dots\\<br /><br />\nabla \frac{\partial f}{\partial x_n}^T\\<br /><br />\end{bmatrix} \mathbf{x-a}\right&gt;\;\;\;(9)$$</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">The matrix in (8) and (9) is the hessian&nbsp; $\mathbf{H}_f$. The hessian provide a second order approximation, a quadratic polynomial approximation near $\mathbf{a}$. This second order approximation is very important, when the first order approximation doesn't tell us the information we need. It is also very important in optimization as we will see in a future post.</span><br />
<span style="font-size: small;"><br /></span>
<span style="font-size: small;">Now with this basic tools we can conquer new areas such as differential geometry, optimization and dynamical systems.</span><br />
<br />
<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'visualexperiments'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();


        </script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
    Disqus.</a></noscript>
</div>


<footer id="footer" class="container-fluid">
    <p class="copyright">VisualExperiments maintained by <a href="https://github.com/pedroth">pedroth</a></p>
    <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
</footer>
</body>
</html>


</html>
