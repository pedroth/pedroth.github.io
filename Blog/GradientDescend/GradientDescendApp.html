<h2>
Gradient Descend and Optimization</h2>
<div>
&nbsp; As always let's start with a problem. Here we have an optimization problem, but what is an optimization problem? Well it is the problem where you have lots of choices and you want to pick one of the choices such that it minimizes/maximizes a certain criteria. More formally we have a function $f: \Omega \rightarrow \mathbb{R}$ which is the criteria function and $\Omega$ is the set of choices, which could be a set of things(chairs, tables , cats , dogs, ...) or could be $\mathbb{R}^{n}$ for some $n \in \mathbb{N}$.</div>
<div>
<br /></div>
<div>
&nbsp; To explain gradient descend I will just consider smooth function(which means that $f$ has second derivatives) over $\mathbb{R}^{2}$. Also I will just consider minimization problems, as we will see ahead maximization problems are similar to solve.</div>
<div>
<br /></div>
<div>
&nbsp; Just to recall we want to find a $\mathbf{x}^{*}\in \mathbb{R}^{2}$ such that it minimizes $f:\mathbb{R}^{2}\rightarrow \mathbb{R}$</div>
<div>
<br /></div>
<div class="separator" style="clear: both; text-align: center;">
<a href="http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Paraboloid_of_Revolution.svg/512px-Paraboloid_of_Revolution.svg.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Paraboloid_of_Revolution.svg/512px-Paraboloid_of_Revolution.svg.png" height="275" width="320" /></a></div>
<div class="separator" style="clear: both; text-align: center;">
figure 1. function of two variables with a global minimum&nbsp;</div>
<div>
&nbsp;&nbsp;</div>
<div>
&nbsp; To solve this problem we could start with a random guess $\mathbf{x_{0}} \in \mathbb{R}^{2}$, then we could choose a direction in which the function decreases and follow that direction, next &nbsp;repeat the process and eventually stop at a minimum. But how to choose the direction, and when to stop? As you can see this is a greedy algorithm, because it always chooses the best direction to go locally, so how to choose this direction locally?</div>
<div>
<br /></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="http://tutorial.math.lamar.edu/Classes/CalcIII/TangentPlanes_files/image001.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://tutorial.math.lamar.edu/Classes/CalcIII/TangentPlanes_files/image001.gif" /></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">fig2. tangent plane approximating surface locally</td></tr>
</tbody></table>
<div>
<br /></div>
<div>
&nbsp; To find such direction first we would find the tangent plane of the function at $\mathbf{x_{0}}$. Which is the graph of the function&nbsp;</div>
<div>
$p:\mathbb{R}^{2}\rightarrow\mathbb{R}$.</div>
<div>
<br /></div>
<div>
$$p(\mathbf{x}) = f(\mathbf{x_{0}}) + &nbsp;\frac{\partial f}{\partial x}(\mathbf{x_{0}}) (x - x_{0}) +&nbsp;\frac{\partial f}{\partial y}(\mathbf{x_{0}}) (y - y_{0})&nbsp;$$<br />
<br />
$$p(\mathbf{x}) = f(\mathbf{x_{0}}) + &nbsp;\mathbf{\nabla} f (\mathbf{x_{0}}) \cdot &nbsp;\mathbf{&nbsp;(x - x_{0})} $$<br />
<br />
(For more about $\mathbf{\nabla} f &nbsp;= \begin{pmatrix}<br />
\frac{\partial f}{\partial x}\\<br />
\frac{\partial f}{\partial y}<br />
\end{pmatrix}$ check <a href="http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/video-lectures/">MIT Multivariate calculus course</a> )</div>
<div>
<br /></div>
<div>
&nbsp; Note that &nbsp;$\mathbf{x_{0}} = \begin{pmatrix}x_{0}\\ y_{0} \end{pmatrix}$.</div>
<div>
<br /></div>
<div>
&nbsp; Then lets see which direction has lowest gradient in the tangent plane, to see that I define $$l(\mathbf{v}) = p(\mathbf{x_{0}} + \mathbf{v})$$</div>
<div>
<br /></div>
<div>
$$\Leftrightarrow &nbsp;l(\mathbf{v}) = f(\mathbf{x_{0}}) + \mathbf{\nabla} f (\mathbf{x_{0}}) \cdot &nbsp;\mathbf{v}$$</div>
<div>
<br /></div>
<div>
&nbsp; Hence we must find $$\underset{\mathbf{v}}{\operatorname{argmin}} l(\mathbf{v}) , \;\;\text{such that }|\mathbf{v}|^{2} \text{ is finite} \;\;\;\; (1)$$</div>
<div>
<br /></div>
<div>
&nbsp; The purpose of constraint is for $l(\mathbf{v})$ to have a minimum. It is clear that in order to minimize (1) we must choose $\mathbf{v}$ that minimizes $ \mathbf{\nabla }f \cdot \mathbf{v}$ this suggests that $\mathbf{v}$ must be parallel with symmetric direction (see <a href="http://pedroth.github.io/visualExperiments/Blog/ProjectionDotProduct/ProjectionDotProduct.html" target="_blank">dot product article</a>) , that is $\mathbf{v} = - \lambda \mathbf{\nabla }f $.</div>
<div>
<br /></div>
<div>
&nbsp; The other problem was when to stop, when do I know that I am in a local max/min? This is simple, in a max/min the tangent plane is parallel to the x,y hence $\mathbf{\nabla }f = \begin{pmatrix}0\\0 \end{pmatrix}$. Now we have a &nbsp;vector that points to decreasing values of &nbsp;$f$ and &nbsp;turns $\mathbf{0}$ at a minimum, and that vector is $\mathbf{\nabla }f$.<br />
<br />
<!--
copy this to https://www.codecogs.com/latex/eqneditor.php
$$\begin{matrix}
\mathbf{x}\leftarrow randomPointOnDomain(\Omega)\\
\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\textbf{while} (|\mathbf{\nabla }f|^{2} >  \epsilon ) \; \textbf{do}:\\
\!\!\!\!\!\!\!\!\mathbf{x} \leftarrow \mathbf{x} - \alpha \mathbf{\nabla }f(\mathbf{x})\\
\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\textbf{return} \; \mathbf{x}
\end{matrix}$$
-->
<center>
	<img src="pseudoCode.png" width="300" height="100">
</center>
</div>
<div>
<br /></div>
<div>
&nbsp; Note that $\epsilon &gt; 0$ close to zero and $0 &lt; \alpha \leq 1$. Also note that as $\mathbf{x_{i}}$ moves towards local min the gradient tends to $\mathbf{0}$. It is important to see that $f(\mathbf{x_{i+1}}) = f(\mathbf{x_{i}}) - \alpha |\mathbf{\nabla }f(\mathbf{x_{i}})|^{2}$ which means $f(\mathbf{x_{i+1}}) &lt; &nbsp;f(\mathbf{x_{i}}) $ since $- \alpha |\mathbf{\nabla }f(\mathbf{x_{i}})|^{2} \leq 0$, which somehow proofs that our algorithm does get to a local minima.</div>
<div>
<br /></div>
<div>
Basically the algorithm just computes equation: $$\mathbf{x_{i+1}} = \mathbf{x_{i}} - \alpha \mathbf{\nabla }f(\mathbf{x_{i}})$$<br />
but this is just the euler step of a differential equation. Then the gradient descent algorithm corresponds to solving $$\mathbf{\dot x(t)} = -\mathbf{\nabla }f(\mathbf{x}(t)) \;\;\;\; (2)$$ And equation (2) just describes a vector field over the domain which points always to the min. Note that to find the max of a function you just invert the sign of the gradient.<br />
<br />
Final remark:<br />
&nbsp; &nbsp;Gradient descend algorithm is a good algorithm for local optimization. The only way to solve a global optimization problem with gradient descend is when the problem has just one minima, or you must run lots of gradient descends with random initial conditions and choose the one that best minimizes the function (this does not guarantee to find the global minima).</div>
<div>
<br /></div>
<div>
Video describing gradient descend:</div>
<div>
<br /></div>
<center>
<div>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cjhTs1DJ4-k" frameborder="0" allowfullscreen></iframe>
</div>
</center>
<br>
<br>
<br>